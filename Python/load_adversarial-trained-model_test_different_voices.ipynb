{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7282852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/454yhlzx6hd15j7rjv4th0lw0000gn/T/ipykernel_88582/1877925962.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipywidgets version: 8.1.2\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------------JUPYTER NOTEBOOK SETTINGS-------------------------------------------------------------------------------------\n",
    "from IPython.core.display import display, HTML                                    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  \n",
    "import IPython.display as ipd\n",
    "\n",
    "import ipywidgets as widgets\n",
    "print(\"ipywidgets version:\", widgets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7798f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Layer, Input, Conv1D, MaxPooling1D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping \n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37e860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='keras.src.saving.saving_lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea67e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up mixed precision policy\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e604f416",
   "metadata": {},
   "source": [
    "### Multi Sub-directory Processing with Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc66b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GradientReversalLayer for adversarial models\n",
    "class GradientReversalLayer(Layer):\n",
    "    def __init__(self, lambda_, **kwargs):\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def call(self, x):\n",
    "        def grad(dy):\n",
    "            return -self.lambda_ * dy\n",
    "        return x, grad\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"lambda_\": self.lambda_\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "809a7adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subdirectory: /Users/ciprian/Desktop/Projects/Smart Plant Pot/Audio/Voice Recognition/Testing Samples/ciprian\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae1861374d34b05a3c8fb4da42ecde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing ciprian:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 442ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Processing subdirectory: /Users/ciprian/Desktop/Projects/Smart Plant Pot/Audio/Voice Recognition/Testing Samples/bianca\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b11f95a5391466ea338ef26b578673f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing bianca:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "   Subdirectory                  Filename Correct Label Predicted Task Label  \\\n",
      "0       ciprian       health_sample_2.wav        health                noise   \n",
      "1       ciprian      battery_sample_1.mp3       battery                noise   \n",
      "2       ciprian  description_sample_1.mp3   description                noise   \n",
      "3       ciprian     greeting_sample_2.mp3      greeting                noise   \n",
      "4       ciprian  description_sample_2.mp3   description                noise   \n",
      "5       ciprian     greeting_sample_1.mp3      greeting                noise   \n",
      "6       ciprian  environment_sample_1.mp3   environment                noise   \n",
      "7       ciprian          sun_sample_1.mp3           sun                noise   \n",
      "8       ciprian  environment_sample_2.mp3   environment                noise   \n",
      "9       ciprian       health_sample_1.mp3        health                noise   \n",
      "10      ciprian        water_sample_4.wav         water                noise   \n",
      "11      ciprian        water_sample_5.wav         water                noise   \n",
      "12      ciprian        water_sample_6.wav         water                noise   \n",
      "13      ciprian        water_sample_2.wav         water                noise   \n",
      "14      ciprian        water_sample_3.wav         water                noise   \n",
      "15      ciprian        water_sample_1.wav         water                noise   \n",
      "16      ciprian                  Accuracy                                      \n",
      "17       bianca  description_sample_1.mp3   description                noise   \n",
      "18       bianca       health_sample_6.wav        health                noise   \n",
      "19       bianca     greeting_sample_2.mp3      greeting                noise   \n",
      "20       bianca     greeting_sample_3.mp3      greeting                noise   \n",
      "21       bianca       health_sample_7.wav        health                noise   \n",
      "22       bianca  description_sample_2.mp3   description                noise   \n",
      "23       bianca       health_sample_5.wav        health                noise   \n",
      "24       bianca     greeting_sample_1.mp3      greeting                noise   \n",
      "25       bianca       health_sample_4.wav        health                noise   \n",
      "26       bianca  description_sample_3.mp3   description                noise   \n",
      "27       bianca  environment_sample_1.mp3   environment                noise   \n",
      "28       bianca  environment_sample_2.mp3   environment                noise   \n",
      "29       bianca       health_sample_1.mp3        health                noise   \n",
      "30       bianca       health_sample_2.mp3        health                noise   \n",
      "31       bianca       health_sample_3.mp3        health                noise   \n",
      "32       bianca       health_sample_9.wav        health                noise   \n",
      "33       bianca       health_sample_8.wav        health                noise   \n",
      "34       bianca                  Accuracy                                      \n",
      "\n",
      "    Task Probability Predicted Gender Label Gender Probability  \n",
      "0           0.757455                 Female           0.157064  \n",
      "1           0.827535                 Female           0.158108  \n",
      "2           0.820953                 Female           0.153522  \n",
      "3           0.792389                 Female           0.164138  \n",
      "4           0.635466                 Female           0.159123  \n",
      "5           0.693090                 Female           0.161858  \n",
      "6           0.876623                 Female           0.160685  \n",
      "7           0.879079                 Female           0.157898  \n",
      "8           0.847601                 Female           0.161007  \n",
      "9           0.839220                 Female           0.158448  \n",
      "10          0.896550                 Female            0.15812  \n",
      "11          0.935221                 Female           0.157723  \n",
      "12          0.938088                 Female           0.158013  \n",
      "13          0.938832                 Female           0.156097  \n",
      "14          0.924142                 Female           0.158809  \n",
      "15          0.866599                 Female            0.15861  \n",
      "16          0.000000                                            \n",
      "17          0.865200                 Female           0.160404  \n",
      "18          0.841603                 Female           0.158398  \n",
      "19          0.847520                 Female           0.159548  \n",
      "20          0.736302                 Female           0.158528  \n",
      "21          0.889035                 Female           0.158551  \n",
      "22          0.955369                 Female           0.158832  \n",
      "23          0.884407                 Female           0.159257  \n",
      "24          0.922320                 Female           0.160484  \n",
      "25          0.898020                 Female           0.153834  \n",
      "26          0.817639                 Female            0.16037  \n",
      "27          0.924363                 Female           0.158556  \n",
      "28          0.876430                 Female           0.162461  \n",
      "29          0.937461                 Female           0.160357  \n",
      "30          0.893133                 Female           0.156416  \n",
      "31          0.856678                 Female           0.157719  \n",
      "32          0.849539                 Female           0.155533  \n",
      "33          0.891780                 Female           0.158038  \n",
      "34          0.000000                                            \n"
     ]
    }
   ],
   "source": [
    "def process_audio_file(file_path, max_length=332):\n",
    "    signal, sr = librosa.load(file_path, sr=16000)\n",
    "    mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13, n_fft=256, hop_length=160, n_mels=32, fmin=0, fmax=8000)\n",
    "    pad_width = max_length - mfccs.shape[1]\n",
    "    if pad_width > 0:\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    return mfccs\n",
    "\n",
    "def predict_audio_file(model, label_encoder, mfccs_padded):\n",
    "    mfccs_padded = mfccs_padded[np.newaxis, ...]  # Add batch dimension\n",
    "    task_output, gender_output = model.predict(mfccs_padded)  # Get the outputs\n",
    "    task_labels = label_encoder.classes_\n",
    "    \n",
    "    task_probabilities_df = pd.DataFrame({'Task Probability': task_output[0]}, index=task_labels)\n",
    "    gender_probabilities_df = pd.DataFrame({'Gender Probability': gender_output[0]}, index=['Female', 'Male'])\n",
    "    \n",
    "    return task_probabilities_df, gender_probabilities_df\n",
    "\n",
    "def process_directory(directory, model, label_encoder):\n",
    "    results = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for sub_dir in dirs:\n",
    "            correct_predictions = 0\n",
    "            total_files = 0\n",
    "            sub_dir_path = os.path.join(root, sub_dir)\n",
    "            print(f\"Processing subdirectory: {sub_dir_path}\")\n",
    "            for file in tqdm(os.listdir(sub_dir_path), desc=f\"Analyzing {sub_dir}\"):\n",
    "                if file.endswith(('.wav', '.mp3')):\n",
    "                    file_path = os.path.join(sub_dir_path, file)\n",
    "                    label_from_filename = file.split('_')[0]  # Assuming the label is the first word in the filename\n",
    "                    mfccs_padded = process_audio_file(file_path)\n",
    "                    task_probabilities_df, gender_probabilities_df = predict_audio_file(model, label_encoder, mfccs_padded)\n",
    "                    \n",
    "                    predicted_task_label = task_probabilities_df['Task Probability'].idxmax()\n",
    "                    highest_task_probability = task_probabilities_df['Task Probability'].max()\n",
    "                    \n",
    "                    predicted_gender_label = gender_probabilities_df['Gender Probability'].idxmax()\n",
    "                    highest_gender_probability = gender_probabilities_df['Gender Probability'].max()\n",
    "                    \n",
    "                    results.append([sub_dir, file, label_from_filename, predicted_task_label, highest_task_probability, predicted_gender_label, highest_gender_probability])\n",
    "                    \n",
    "                    if predicted_task_label == label_from_filename:\n",
    "                        correct_predictions += 1\n",
    "                    total_files += 1\n",
    "            \n",
    "            accuracy = (correct_predictions / total_files) * 100 if total_files > 0 else 0\n",
    "            results.append([sub_dir, \"Accuracy\", \"\", \"\", accuracy, \"\", \"\"])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['Subdirectory', 'Filename', 'Correct Label', 'Predicted Task Label', 'Task Probability', 'Predicted Gender Label', 'Gender Probability'])\n",
    "    return results_df\n",
    "\n",
    "# Load the pre-trained model and label encoder\n",
    "custom_objects = {\"GradientReversalLayer\": GradientReversalLayer}\n",
    "\n",
    "# Load the model\n",
    "model = load_model(\"saved_data/models/adversarial-training_medium-masked_mobilenetv3small-finetuned_v3/adversarial-training_mobilenetv3small_fine-tuning.keras\", custom_objects=custom_objects)\n",
    "all_labels = ['battery', 'description', 'environment', 'greeting', 'health', 'noise', 'nutrition', 'silence', 'sun', 'water']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Specify the directory containing subdirectories with audio files\n",
    "directory = '/Users/ciprian/Desktop/Projects/Smart Plant Pot/Audio/Voice Recognition/Testing Samples'\n",
    "\n",
    "# Process the directory and get predictions along with accuracy\n",
    "predictions_df = process_directory(directory, model, label_encoder)\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GradientReversalLayer for adversarial models (if any exist)\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class GradientReversalLayer(Layer):\n",
    "    def __init__(self, lambda_=1.0, **kwargs):\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def call(self, x):\n",
    "        def grad(dy):\n",
    "            return -self.lambda_ * dy\n",
    "        return x, grad\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"lambda_\": self.lambda_})\n",
    "        return config\n",
    "\n",
    "def process_audio_file(file_path, max_length=332):\n",
    "    # Load the audio file with librosa, handle both mp3 and wav formats\n",
    "    signal, sr = librosa.load(file_path, sr=16000)\n",
    "    mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13, n_fft=256, hop_length=160, n_mels=32, fmin=0, fmax=8000)\n",
    "    pad_width = max_length - mfccs.shape[1]\n",
    "    if pad_width > 0:\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    return mfccs\n",
    "\n",
    "def evaluate_model_directory(model_path, test_directory, label_encoder):\n",
    "    # Function to evaluate a model using audio files in the test directory\n",
    "    try:\n",
    "        custom_objects = {\"GradientReversalLayer\": GradientReversalLayer}\n",
    "        model = load_model(model_path, custom_objects=custom_objects)\n",
    "    except:\n",
    "        model = load_model(model_path)  # For models without adversarial training\n",
    "    \n",
    "    results = []\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    for root, _, files in os.walk(test_directory):\n",
    "        for file in tqdm(files, desc=f\"Processing {os.path.basename(model_path)}\"):\n",
    "            if file.endswith(('.wav', '.mp3')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                label_from_filename = file.split('_')[0]  # Assuming the label is the first word in the filename\n",
    "                mfccs_padded = process_audio_file(file_path)\n",
    "                mfccs_padded = mfccs_padded[np.newaxis, ...]  # Add batch dimension\n",
    "                \n",
    "                predictions = model.predict(mfccs_padded)\n",
    "                \n",
    "                if isinstance(predictions, list):  # Adversarial model\n",
    "                    y_pred_task = predictions[0]\n",
    "                else:  # Non-adversarial model\n",
    "                    y_pred_task = predictions\n",
    "                \n",
    "                predicted_label = label_encoder.inverse_transform(np.argmax(y_pred_task, axis=1))[0]\n",
    "                \n",
    "                y_true.append(label_from_filename)\n",
    "                y_pred.append(predicted_label)\n",
    "                \n",
    "                results.append([file, label_from_filename, predicted_label])\n",
    "    \n",
    "    command_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    command_precision = precision_score(y_true, y_pred, average='weighted', zero_division=0) * 100\n",
    "    command_recall = recall_score(y_true, y_pred, average='weighted', zero_division=0) * 100\n",
    "    command_f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
    "    \n",
    "    return results, command_accuracy, command_precision, command_recall, command_f1\n",
    "\n",
    "# Set the path to the models directory and the test data directory\n",
    "models_directory = 'saved_data/models'\n",
    "test_directory = '/Users/ciprian/Desktop/Projects/Smart Plant Pot/Audio/Voice Recognition/Testing Samples'\n",
    "\n",
    "# Label encoder setup\n",
    "all_labels = ['battery', 'description', 'environment', 'greeting', 'health', 'noise', 'nutrition', 'silence', 'sun', 'water']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Iterate through subdirectories in the test directory and evaluate models\n",
    "overall_results = []\n",
    "subdir_results = {}\n",
    "\n",
    "for subdir, _, _ in os.walk(test_directory):\n",
    "    subdir_name = os.path.basename(subdir)\n",
    "    if subdir_name not in subdir_results:\n",
    "        subdir_results[subdir_name] = []\n",
    "    \n",
    "    for model_subdir, _, model_files in os.walk(models_directory):\n",
    "        for model_file in model_files:\n",
    "            if model_file.endswith('.keras'):\n",
    "                model_path = os.path.join(model_subdir, model_file)\n",
    "                model_folder = os.path.basename(model_subdir)\n",
    "                \n",
    "                model_results, command_accuracy, command_precision, command_recall, command_f1 = evaluate_model_directory(model_path, subdir, label_encoder)\n",
    "                \n",
    "                result_entry = {\n",
    "                    'Model': model_folder,\n",
    "                    'Command Accuracy (%)': f\"{command_accuracy:.2f}\",\n",
    "                    'Command Precision (%)': f\"{command_precision:.2f}\",\n",
    "                    'Command Recall (%)': f\"{command_recall:.2f}\",\n",
    "                    'Command F1 (%)': f\"{command_f1:.2f}\"\n",
    "                }\n",
    "                subdir_results[subdir_name].append(result_entry)\n",
    "                overall_results.append(result_entry)\n",
    "\n",
    "# Display performance DataFrames for each subdirectory\n",
    "for subdir_name, results in subdir_results.items():\n",
    "    if results:  # Only display if there are results\n",
    "        print(f\"Performance for {subdir_name}:\")\n",
    "        subdir_df = pd.DataFrame(results)\n",
    "        ipd.display(ipd.HTML(subdir_df.to_html(index=False)))\n",
    "\n",
    "# Create a summary DataFrame with the best model for each subdirectory\n",
    "summary_results = []\n",
    "\n",
    "for subdir_name, results in subdir_results.items():\n",
    "    subdir_df = pd.DataFrame(results)\n",
    "    if not subdir_df.empty and 'Command Accuracy (%)' in subdir_df.columns and 'Command F1 (%)' in subdir_df.columns:\n",
    "        best_accuracy_model = subdir_df.loc[subdir_df['Command Accuracy (%)'].astype(float).idxmax()]\n",
    "        best_f1_model = subdir_df.loc[subdir_df['Command F1 (%)'].astype(float).idxmax()]\n",
    "        summary_results.append({\n",
    "            'Subdirectory': subdir_name,\n",
    "            'Best Accuracy Model': best_accuracy_model['Model'],\n",
    "            'Best Accuracy (%)': best_accuracy_model['Command Accuracy (%)'],\n",
    "            'Best F1 Model': best_f1_model['Model'],\n",
    "            'Best F1 Score (%)': best_f1_model['Command F1 (%)']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "\n",
    "# Display the final summary DataFrame\n",
    "print(\"Overall Performance Summary:\")\n",
    "ipd.display(ipd.HTML(summary_df.to_html(index=False)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.14",
   "language": "python",
   "name": "tf-2.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
