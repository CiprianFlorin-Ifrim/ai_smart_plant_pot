{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c82a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------JUPYTER NOTEBOOK SETTINGS-------------------------------------------------------------------------------------\n",
    "from IPython.core.display import display, HTML                                    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79770105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Reshape, UpSampling2D, Conv2D, GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping \n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc307eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load('saved_data/train_data.joblib')\n",
    "x_val, y_val = load('saved_data/val_data.joblib')\n",
    "x_test, y_test = load('saved_data/test_data.joblib')\n",
    "print(\"All data has been loaded properly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d66f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data:\", x_train.shape, x_train.dtype)\n",
    "print(\"Validation data:\", x_val.shape, x_val.dtype)\n",
    "print(\"Test data:\", x_test.shape, x_test.dtype)\n",
    "\n",
    "# Check for any NaN or inf values in your dataset\n",
    "print(\"NaNs in train:\", np.isnan(x_train).any())\n",
    "print(\"NaNs in validation:\", np.isnan(x_val).any())\n",
    "print(\"NaNs in test:\", np.isnan(x_test).any())\n",
    "\n",
    "print(\"Infs in train:\", np.isinf(x_train).any())\n",
    "print(\"Infs in validation:\", np.isinf(x_val).any())\n",
    "print(\"Infs in test:\", np.isinf(x_test).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONEHOT ENCODING THE LABELS\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_encoded)\n",
    "y_val_onehot = to_categorical(y_val_encoded)\n",
    "y_test_onehot = to_categorical(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_weights(weights_dir, file_pattern):\n",
    "    \"\"\"Load the latest weights based on the file modification time.\"\"\"\n",
    "    # List all files in the directory that match the pattern\n",
    "    all_weights = [os.path.join(weights_dir, f) for f in os.listdir(weights_dir) if file_pattern in f]\n",
    "    # Find the most recent file by sorting based on modification time\n",
    "    latest_weights = max(all_weights, key=os.path.getmtime, default=None)\n",
    "    if latest_weights:\n",
    "        print(f\"Loading weights from {latest_weights}\")\n",
    "        return latest_weights\n",
    "    else:\n",
    "        print(\"No weights file found.\")\n",
    "        return None\n",
    "\n",
    "class SaveWeightsCallback(Callback):\n",
    "    def __init__(self, save_freq, filepath):\n",
    "        super(SaveWeightsCallback, self).__init__()\n",
    "        self.save_freq = save_freq\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Check if the current epoch number is a multiple of the save frequency\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            self.model.save_weights(self.filepath.format(epoch=epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up mixed precision policy\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileNetV2 pre-trained on ImageNet\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "\n",
    "# Freeze the convolutional base\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(13, 332)),  # Explicit Input layer\n",
    "    Reshape((13, 332, 1)),   # Reshape to add a channel dimension\n",
    "    UpSampling2D(size=(8, 1)),  # Upsample to increase height\n",
    "    Conv2D(3, (3, 3), activation='relu', padding='same'),  # Convert to 3 channels\n",
    "    tf.keras.layers.Resizing(96, 96),  # Resize to the expected input shape of MobileNetV2\n",
    "    base_model,  # Add the pre-trained MobileNetV2\n",
    "    GlobalAveragePooling2D(),  # Pooling layer to reduce spatial dimensions\n",
    "    Dropout(0.2),  # Dropout for regularization\n",
    "    Dense(64, activation='relu'),  # Fully connected layer\n",
    "    Dropout(0.5),  # Another dropout for regularization\n",
    "    Dense(y_train_onehot.shape[1], activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Configure the optimizer, loss, and metrics\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setup callbacks\n",
    "early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.000001, verbose=1)\n",
    "weights_saver = SaveWeightsCallback(save_freq=50, filepath='saved_data/mobilenetv2_finetuned_weights_epoch_{epoch}.weights.h5')\n",
    "\n",
    "\n",
    "# Segment-based training setup\n",
    "num_epochs_per_stage = 50\n",
    "total_epochs = 500\n",
    "current_epoch = 0\n",
    "all_history = []\n",
    "\n",
    "while current_epoch < total_epochs:\n",
    "    try:\n",
    "        # Load the latest model weights if available\n",
    "        try:\n",
    "            latest_weights_file = load_latest_weights('saved_data', '.weights.h5')\n",
    "            if latest_weights_file:\n",
    "                model.load_weights(latest_weights_file)\n",
    "        except Exception as e:\n",
    "            print(\"Loading weights failed:\", e)\n",
    "        \n",
    "        # Train the model for a stage\n",
    "        history = model.fit(\n",
    "            x_train,\n",
    "            y_train_onehot, \n",
    "            epochs=current_epoch + num_epochs_per_stage,\n",
    "            batch_size=512,\n",
    "            validation_data=(x_val, y_val_onehot),\n",
    "            callbacks=[weights_saver, early_stopping_monitor, reduce_lr_on_plateau],\n",
    "            initial_epoch=current_epoch,\n",
    "            verbose=1  \n",
    "        )\n",
    "        \n",
    "        # Append segment history to the total history\n",
    "        all_history.append(history.history)\n",
    "        \n",
    "        # Update the current epoch count\n",
    "        current_epoch += len(history.history['loss'])\n",
    "        \n",
    "        # Optionally perform garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Check if early stopping was triggered\n",
    "        if early_stopping_monitor.stopped_epoch > 0:\n",
    "            print(f\"Early stopping triggered at epoch {current_epoch}\")\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during training:\", e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd130fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model.save('saved_data/mobilenetv2_finetuned_model.keras')\n",
    "\n",
    "# Concatenate all history segments into one dictionary if there are any segments\n",
    "if all_history:\n",
    "    final_history = {key: np.concatenate([seg[key] for seg in all_history]) for key in all_history[0]}\n",
    "    # Save the final training history\n",
    "    dump(history.history, 'saved_data/mobilenetv2_finetuned_training_history.joblib')\n",
    "else:\n",
    "    print(\"No training history was recorded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
